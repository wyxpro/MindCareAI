import { useState, useRef, useEffect, useCallback } from 'react';
import { useAuth } from '@/contexts/AuthContext';
import { Dialog, DialogContent, DialogHeader, DialogTitle } from '@/components/ui/dialog';
import {
  createAssessment,
  updateAssessment,
  ragRetrieval,
  multimodalAnalysis,
  speechRecognition,
  multimodalFusion,
  getAssessments,
  chatCompletion
} from '@/db/api';
import { toast } from 'sonner';
import { AssessmentHeader, MessageList, ChatInput, EmotionCamera, AssessmentReport } from '@/components/assessment/index';
import { convertWebmToWav } from '@/utils/audio';
import type { Message } from '@/components/assessment/MessageList';
import type { Assessment, ChatMessage, MultimodalData } from '@/types';



export default function EnhancedAssessmentPage() {
  const { user } = useAuth();
  const [messages, setMessages] = useState<Message[]>([
    {
      role: 'assistant',
      content: 'ä½ å¥½!æˆ‘æ˜¯çµæ„ˆAIåŠ©æ‰‹ã€‚æˆ‘ä¼šé€šè¿‡ä¸“ä¸šçš„è¯„ä¼°é‡è¡¨ä¸ä½ è¿›è¡Œå¯¹è¯,äº†è§£ä½ çš„å¿ƒç†çŠ¶æ€ã€‚è¯·æ”¾æ¾,éšæ„åˆ†äº«ä½ çš„æ„Ÿå—ã€‚ä½ æœ€è¿‘ä¸¤å‘¨çš„å¿ƒæƒ…æ€ä¹ˆæ ·?',
      timestamp: new Date(),
    },
  ]);
  const [inputText, setInputText] = useState('');
  const [loading, setLoading] = useState(false);
  const [currentAssessment, setCurrentAssessment] = useState<Assessment | null>(null);
  const [reportDialogOpen, setReportDialogOpen] = useState(false);
  const [assessmentType, setAssessmentType] = useState('PHQ-9');
  const [isRecording, setIsRecording] = useState(false);
  const [isCameraOpen, setIsCameraOpen] = useState(false);
  const [analysisProgress, setAnalysisProgress] = useState(0);
  const [multimodalData, setMultimodalData] = useState<MultimodalData>({});
  const [historicalAssessments, setHistoricalAssessments] = useState<Assessment[]>([]);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);

  const serializeConversationHistory = (items: Message[]) => {
    // ä»…ä¿å­˜å¯åºåˆ—åŒ–å­—æ®µï¼Œé¿å… Date/å¤æ‚å¯¹è±¡å¯¼è‡´å…¥åº“å¼‚å¸¸
    return items.map((item) => ({
      role: item.role,
      content: item.content,
      timestamp: item.timestamp instanceof Date
        ? item.timestamp.toISOString()
        : new Date(item.timestamp).toISOString(),
    }));
  };


  useEffect(() => {
    if (user && !currentAssessment) {
      initAssessment();
      loadHistoricalData();
    }
  }, [user]);

  const initAssessment = async () => {
    if (!user) return;
    try {
      const assessment = await createAssessment({
        assessment_type: 'multimodal',
        conversation_history: [],
      });
      setCurrentAssessment(assessment);
    } catch (error) {
      console.error('åˆ›å»ºè¯„ä¼°å¤±è´¥:', error);
    }
  };

  const loadHistoricalData = async () => {
    if (!user) return;
    try {
      const data = await getAssessments(10);
      setHistoricalAssessments(data);
    } catch (error) {
      console.error('åŠ è½½å†å²æ•°æ®å¤±è´¥:', error);
    }
  };

  // æ–‡æœ¬æ¶ˆæ¯å‘é€ - ä½¿ç”¨RAGæ£€ç´¢
  const handleSendMessage = useCallback(async () => {
    if (!inputText.trim() || loading) return;

    const userMessage: Message = {
      role: 'user',
      content: inputText,
      type: 'text',
      timestamp: new Date(),
    };

    setMessages(prev => [...prev, userMessage]);
    setInputText('');
    setLoading(true);
    setAnalysisProgress(20);

    try {
      // æ„å»ºå¯¹è¯å†å²
      const chatHistory: ChatMessage[] = messages
        .filter(m => m.role !== 'system')
        .map(m => ({
          role: m.role,
          content: m.content,
        }));

      // ä½¿ç”¨RAGæ£€ç´¢è¿›è¡Œä¸»åŠ¨å¼å¯¹è¯
      const response = await ragRetrieval(inputText, chatHistory, assessmentType);
      setAnalysisProgress(60);

      if (response?.choices?.[0]?.delta?.content) {
        const aiMessage: Message = {
          role: 'assistant',
          content: response.choices[0].delta.content,
          timestamp: new Date(),
        };
        setMessages(prev => [...prev, aiMessage]);

        // æ–‡æœ¬æƒ…ç»ªåˆ†æ
        const textAnalysis = await analyzeTextEmotion(inputText);
        setMultimodalData(prev => ({ ...prev, text_analysis: textAnalysis }));
        setAnalysisProgress(80);

        // æ›´æ–°è¯„ä¼°è®°å½•
        if (currentAssessment) {
          await updateAssessment(currentAssessment.id, {
            conversation_history: serializeConversationHistory([...messages, userMessage, aiMessage]),
            text_input: inputText,
          });
        }
      }

      setAnalysisProgress(100);
    } catch (error) {
      console.error('å‘é€æ¶ˆæ¯å¤±è´¥:', error);
      toast.error('å‘é€å¤±è´¥,è¯·é‡è¯•');
    } finally {
      setLoading(false);
      setTimeout(() => setAnalysisProgress(0), 500);
    }
  }, [inputText, loading, messages, assessmentType, currentAssessment]);

  // æ–‡æœ¬æƒ…ç»ªåˆ†æ
  const analyzeTextEmotion = async (text: string) => {
    // è´Ÿé¢å…³é”®è¯æ£€æµ‹
    const negativeKeywords = ['æ²¡ç”¨', 'æ´»ç€æ²¡æ„æ€', 'ç´¯', 'ç—›è‹¦', 'ç»æœ›', 'å­¤ç‹¬', 'æ— åŠ©', 'å¤±çœ ', 'ç„¦è™‘', 'æŠ‘éƒ'];
    const keywordCount = negativeKeywords.filter(kw => text.includes(kw)).length;

    // ç®€å•çš„æƒ…ç»ªè¯„åˆ†(å®é™…åº”è¯¥ç”¨NLPæ¨¡å‹)
    const emotionScore = Math.min(keywordCount * 1.5 + Math.random() * 2, 10);

    return {
      emotion_score: emotionScore,
      negative_keywords: negativeKeywords.filter(kw => text.includes(kw)),
      text_length: text.length,
    };
  };

  // å›¾ç‰‡ä¸Šä¼ å’Œåˆ†æ
  const handleImageUpload = useCallback(async (e: React.ChangeEvent<HTMLInputElement>) => {
    const file = e.target.files?.[0];
    if (!file) return;

    if (file.size > 10 * 1024 * 1024) {
      toast.error('å›¾ç‰‡å¤§å°ä¸èƒ½è¶…è¿‡10MB');
      return;
    }

    setLoading(true);
    setAnalysisProgress(20);

    try {
      const reader = new FileReader();
      reader.onload = async (event) => {
        const base64 = event.target?.result as string;

        const userMessage: Message = {
          role: 'user',
          content: '[å·²ä¸Šä¼ å›¾ç‰‡è¿›è¡Œæƒ…ç»ªåˆ†æ]',
          type: 'image',
          timestamp: new Date(),
        };
        setMessages(prev => [...prev, userMessage]);
        setAnalysisProgress(40);

        // ä½¿ç”¨ api.ts ä¸­çš„ multimodalAnalysis
        const response = await multimodalAnalysis([
          {
            role: 'user',
            content: [
              { type: 'text', text: 'è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„æƒ…ç»ªè¡¨ç°,åŒ…æ‹¬é¢éƒ¨è¡¨æƒ…ã€è‚¢ä½“è¯­è¨€ã€ç¯å¢ƒæ°›å›´ç­‰,ç»™å‡ºä¸“ä¸šçš„æƒ…ç»ªè¯„ä¼°å’ŒæŠ‘éƒé£é™©åˆ†æã€‚' },
              { type: 'image_url', image_url: { url: base64 } },
            ],
          },
        ]);

        setAnalysisProgress(70);

        // è§£æå“åº” (NestJS éæµå¼è¿”å›æˆ–ç»“æ„åŒ–è¿”å›)
        let analysis = response?.choices?.[0]?.message?.content || response?.choices?.[0]?.delta?.content || '';

        if (analysis) {
          const aiMessage: Message = {
            role: 'assistant',
            content: analysis,
            timestamp: new Date(),
            analysis: { type: 'image', content: analysis },
          };
          setMessages(prev => [...prev, aiMessage]);

          // æå–æƒ…ç»ªåˆ†æ•°(ç®€åŒ–ç‰ˆ)
          const imageAnalysis = {
            emotion_score: 5 + Math.random() * 3,
            analysis_text: analysis,
          };
          setMultimodalData(prev => ({ ...prev, image_analysis: imageAnalysis }));
          setAnalysisProgress(90);

          if (currentAssessment) {
            await updateAssessment(currentAssessment.id, {
              conversation_history: serializeConversationHistory([...messages, userMessage, aiMessage]),
              image_input_url: base64,
            });
          }
        }

        setAnalysisProgress(100);
        setLoading(false);
        setTimeout(() => setAnalysisProgress(0), 500);
      };
      reader.readAsDataURL(file);
    } catch (error) {
      console.error('å›¾ç‰‡åˆ†æå¤±è´¥:', error);
      toast.error('å›¾ç‰‡åˆ†æå¤±è´¥');
      setLoading(false);
      setAnalysisProgress(0);
    }
  }, [messages, currentAssessment]);

  // è¯­éŸ³å½•åˆ¶
  const handleStartRecording = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
        }
      });
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm',
      });
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        audioChunksRef.current.push(event.data);
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        await processAudioRecording(audioBlob);
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.start();
      setIsRecording(true);
      toast.success('å¼€å§‹å½•éŸ³...');
    } catch (error) {
      console.error('å½•éŸ³å¤±è´¥:', error);
      toast.error('æ— æ³•è®¿é—®éº¦å…‹é£');
    }
  }, [assessmentType, messages]);

  const handleStopRecording = useCallback(() => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  }, [isRecording]);

  const processAudioRecording = async (audioBlob: Blob) => {
    setLoading(true);
    setAnalysisProgress(20);

    try {
      // è½¬æ¢webmä¸ºwav
      const wavBlob = await convertWebmToWav(audioBlob);
      setAnalysisProgress(30);

      setAnalysisProgress(40);

      const userMessage: Message = {
        role: 'user',
        content: '[è¯­éŸ³è¾“å…¥]',
        type: 'voice',
        timestamp: new Date(),
      };
      setMessages(prev => [...prev, userMessage]);

      // ä½¿ç”¨ api.ts ä¸­çš„ speechRecognition
      const data = await speechRecognition(wavBlob, 'wav', 'zh');

      setAnalysisProgress(60);

      if (data?.text) {
        const recognizedText = data.text;

        // è¯­éŸ³æƒ…ç»ªåˆ†æ
        const voiceAnalysis = {
          emotion_score: 4 + Math.random() * 4,
          recognized_text: recognizedText,
          duration: wavBlob.size / 16000, // ä¼°ç®—æ—¶é•¿
        };
        setMultimodalData(prev => ({ ...prev, voice_analysis: voiceAnalysis }));

        // ä½¿ç”¨ chatCompletion
        const chatResponse = await chatCompletion([
          {
            role: 'system',
            content: `ä½ æ˜¯çµæ„ˆAIåŠ©æ‰‹,æ­£åœ¨è¿›è¡Œ${assessmentType}æŠ‘éƒè¯„ä¼°ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¯­éŸ³è¾“å…¥è¿›è¡Œä¸“ä¸šçš„å¿ƒç†è¯„ä¼°ã€‚`,
          },
          ...messages.map(m => ({
            role: m.role,
            content: m.content,
          })),
          {
            role: 'user',
            content: recognizedText,
          },
        ]);

        setAnalysisProgress(80);

        let aiResponse = chatResponse?.choices?.[0]?.message?.content || chatResponse?.choices?.[0]?.delta?.content || '';

        if (aiResponse) {
          const aiMessage: Message = {
            role: 'assistant',
            content: `[è¯­éŸ³è¯†åˆ«: ${recognizedText}]\n\n${aiResponse}`,
            timestamp: new Date(),
          };
          setMessages(prev => [...prev, aiMessage]);
        }
      }

      setAnalysisProgress(100);
      setLoading(false);
      setTimeout(() => setAnalysisProgress(0), 500);
    } catch (error) {
      console.error('è¯­éŸ³å¤„ç†å¤±è´¥:', error);
      toast.error('è¯­éŸ³å¤„ç†å¤±è´¥');
      setLoading(false);
      setAnalysisProgress(0);
    }
  };

  // æ‘„åƒå¤´é¢éƒ¨è¡¨æƒ…è¯†åˆ« - ä½¿ç”¨æ–°ç»„ä»¶
  const handleOpenCamera = useCallback(() => {
    setIsCameraOpen(true);
  }, []);

  const handleCloseCamera = useCallback(() => {
    setIsCameraOpen(false);
  }, []);

  const handleEmotionDetected = useCallback((emotion: string, confidence: number) => {
    // è®°å½•æ£€æµ‹åˆ°çš„è¡¨æƒ…
    const emotionData = {
      emotion,
      confidence,
      timestamp: new Date().toISOString(),
    };

    setMultimodalData(prev => ({
      ...prev,
      facial_emotion: emotionData,
    }));

    // æ·»åŠ åˆ° message å†å²? (æ ¹æ®åŸé€»è¾‘)
    // åŸé€»è¾‘åªæ˜¯ toast
    const emotionLabels: Record<string, string> = {
      happy: 'å¿«ä¹', sad: 'æ‚²ä¼¤', angry: 'æ„¤æ€’', fear: 'ææƒ§',
      surprise: 'æƒŠè®¶', disgust: 'åŒæ¶', neutral: 'ä¸­æ€§',
      embarrassed: 'å°´å°¬', anxious: 'ç„¦è™‘', calm: 'å¹³é™',
    };

    const emotionEmojis: Record<string, string> = {
      happy: 'ğŸ˜Š', sad: 'ğŸ˜¢', angry: 'ğŸ˜ ', fear: 'ğŸ˜¨',
      surprise: 'ğŸ˜²', disgust: 'ğŸ¤¢', neutral: 'ğŸ˜',
      embarrassed: 'ğŸ˜…', anxious: 'ğŸ˜°', calm: 'ğŸ˜Œ',
    };

    const label = emotionLabels[emotion] || emotion;
    const emoji = emotionEmojis[emotion] || 'ğŸ˜';

    toast.success(`æ£€æµ‹åˆ°è¡¨æƒ…: ${emoji} ${label} (${confidence}%)`);
  }, []);


  // ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š
  const handleGenerateReport = useCallback(async () => {
    if (!currentAssessment || messages.length < 5) {
      toast.error('å¯¹è¯å†…å®¹å¤ªå°‘,è¯·ç»§ç»­äº¤æµ');
      return;
    }

    setLoading(true);
    setAnalysisProgress(20);

    try {
      // å¤šæ¨¡æ€èåˆåˆ†æ
      const fusionResult = await multimodalFusion({
        textInput: inputText,
        enableAI: true,
        // multimodalData åŒ…å«è¿‡å¾€åˆ†æï¼Œä½†åœ¨å½“å‰ DTO ä¸­æœªå®šä¹‰
        // æˆ‘ä»¬å¯ä»¥å°†å…¶è®°å½•åˆ°æ—¥å¿—æˆ–ä½œä¸ºæ‰©å±•å‚æ•°ï¼ˆå¦‚æœåç«¯æ”¯æŒï¼‰
      });
      console.log('èåˆåˆ†æå‚è€ƒæ•°æ®:', multimodalData);
      setAnalysisProgress(80);

      if (fusionResult.success) {
        toast.success('è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ');
        await loadHistoricalData();
        setReportDialogOpen(true);

        // é‡æ–°åŠ è½½è¯„ä¼°æ•°æ®
        const updatedAssessment = await getAssessments(1);
        if (updatedAssessment[0]) {
          setCurrentAssessment(updatedAssessment[0]);
        }
      }

      setAnalysisProgress(100);
    } catch (error) {
      console.error('ç”ŸæˆæŠ¥å‘Šå¤±è´¥:', error);
      toast.error('ç”ŸæˆæŠ¥å‘Šå¤±è´¥');
      setAnalysisProgress(0);
    } finally {
      setLoading(false);
      setTimeout(() => setAnalysisProgress(0), 500);
    }
  }, [currentAssessment, messages, inputText, multimodalData]);

  return (
    <div className="flex flex-col h-screen bg-background">
      <AssessmentHeader
        assessmentType={assessmentType}
        setAssessmentType={setAssessmentType}
        loading={loading}
        messagesCount={messages.length}
        onGenerateReport={handleGenerateReport}
        analysisProgress={analysisProgress}
      />

      <MessageList messages={messages} loading={loading} />

      {/* æ‘„åƒå¤´é¢„è§ˆ - æ”¾åœ¨ä¸­é—´å±‚æˆ–æµ®å±‚ */}
      {isCameraOpen && (
        <div className="px-4 py-2">
          <EmotionCamera
            onClose={handleCloseCamera}
            onEmotionDetected={handleEmotionDetected}
          />
        </div>
      )}

      <ChatInput
        inputText={inputText}
        setInputText={setInputText}
        loading={loading}
        isRecording={isRecording}
        onSendMessage={handleSendMessage}
        onImageUpload={handleImageUpload}
        onToggleRecording={isRecording ? handleStopRecording : handleStartRecording}
        onOpenCamera={handleOpenCamera}
      />

      {/* è¯„ä¼°æŠ¥å‘Šå¯¹è¯æ¡† */}
      <Dialog open={reportDialogOpen} onOpenChange={setReportDialogOpen}>
        <DialogContent className="max-w-6xl max-h-[90vh] overflow-y-auto">
          <DialogHeader>
            <DialogTitle>å¿ƒç†è¯„ä¼°æŠ¥å‘Š</DialogTitle>
          </DialogHeader>
          {currentAssessment && currentAssessment.risk_level !== undefined && (
            <AssessmentReport
              assessment={{
                ...currentAssessment,
                risk_level: currentAssessment.risk_level || 0,
                score: currentAssessment.score || 0,
              }}
              historicalData={historicalAssessments.map(a => ({
                date: a.created_at,
                score: a.score || 0,
                risk_level: a.risk_level || 0,
              }))}
              onClose={() => setReportDialogOpen(false)}
            />
          )}
        </DialogContent>
      </Dialog>
    </div>
  );
}
