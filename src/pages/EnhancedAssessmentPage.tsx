import { useState, useRef, useEffect } from 'react';
import { useAuth } from '@/contexts/AuthContext';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import { Textarea } from '@/components/ui/textarea';
import { Badge } from '@/components/ui/badge';
import { ScrollArea } from '@/components/ui/scroll-area';
import { Dialog, DialogContent, DialogHeader, DialogTitle } from '@/components/ui/dialog';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from '@/components/ui/select';
import { Progress } from '@/components/ui/progress';
import { createAssessment, updateAssessment, ragRetrieval, multimodalAnalysis, speechRecognition, multimodalFusion, getAssessments } from '@/db/api';
import { supabase } from '@/db/supabase';
import { toast } from 'sonner';
import { Send, Mic, Image as ImageIcon, Video, FileText, Loader2, Brain, Camera, StopCircle, Play } from 'lucide-react';
import type { Assessment, ChatMessage } from '@/types';
import AssessmentReport from '@/components/assessment/AssessmentReport';
import EmotionCamera from '@/components/assessment/EmotionCamera';
import { convertWebmToWav, blobToBase64 } from '@/utils/audio';

interface Message {
  role: 'user' | 'assistant' | 'system';
  content: string;
  type?: 'text' | 'image' | 'voice' | 'video';
  timestamp: Date;
  analysis?: any;
}

export default function EnhancedAssessmentPage() {
  const { user } = useAuth();
  const [messages, setMessages] = useState<Message[]>([
    {
      role: 'assistant',
      content: 'ä½ å¥½!æˆ‘æ˜¯çµæ„ˆAIåŠ©æ‰‹ã€‚æˆ‘ä¼šé€šè¿‡ä¸“ä¸šçš„è¯„ä¼°é‡è¡¨ä¸ä½ è¿›è¡Œå¯¹è¯,äº†è§£ä½ çš„å¿ƒç†çŠ¶æ€ã€‚è¯·æ”¾æ¾,éšæ„åˆ†äº«ä½ çš„æ„Ÿå—ã€‚ä½ æœ€è¿‘ä¸¤å‘¨çš„å¿ƒæƒ…æ€ä¹ˆæ ·?',
      timestamp: new Date(),
    },
  ]);
  const [inputText, setInputText] = useState('');
  const [loading, setLoading] = useState(false);
  const [currentAssessment, setCurrentAssessment] = useState<Assessment | null>(null);
  const [reportDialogOpen, setReportDialogOpen] = useState(false);
  const [assessmentType, setAssessmentType] = useState('PHQ-9');
  const [isRecording, setIsRecording] = useState(false);
  const [isCameraOpen, setIsCameraOpen] = useState(false);
  const [analysisProgress, setAnalysisProgress] = useState(0);
  const [multimodalData, setMultimodalData] = useState<any>({});
  const [historicalAssessments, setHistoricalAssessments] = useState<any[]>([]);
  
  const scrollRef = useRef<HTMLDivElement>(null);
  const fileInputRef = useRef<HTMLInputElement>(null);
  const videoRef = useRef<HTMLVideoElement>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);

  useEffect(() => {
    if (scrollRef.current) {
      scrollRef.current.scrollTop = scrollRef.current.scrollHeight;
    }
  }, [messages]);

  useEffect(() => {
    if (user && !currentAssessment) {
      initAssessment();
      loadHistoricalData();
    }
  }, [user]);

  const initAssessment = async () => {
    if (!user) return;
    try {
      const assessment = await createAssessment({
        user_id: user.id,
        assessment_type: 'multimodal',
        conversation_history: [],
      });
      setCurrentAssessment(assessment);
    } catch (error) {
      console.error('åˆ›å»ºè¯„ä¼°å¤±è´¥:', error);
    }
  };

  const loadHistoricalData = async () => {
    if (!user) return;
    try {
      const data = await getAssessments(user.id, 10);
      setHistoricalAssessments(data);
    } catch (error) {
      console.error('åŠ è½½å†å²æ•°æ®å¤±è´¥:', error);
    }
  };

  // æ–‡æœ¬æ¶ˆæ¯å‘é€ - ä½¿ç”¨RAGæ£€ç´¢
  const handleSendMessage = async () => {
    if (!inputText.trim() || loading) return;

    const userMessage: Message = {
      role: 'user',
      content: inputText,
      type: 'text',
      timestamp: new Date(),
    };

    setMessages(prev => [...prev, userMessage]);
    setInputText('');
    setLoading(true);
    setAnalysisProgress(20);

    try {
      // æ„å»ºå¯¹è¯å†å²
      const chatHistory: ChatMessage[] = messages
        .filter(m => m.role !== 'system')
        .map(m => ({
          role: m.role,
          content: m.content,
        }));

      // ä½¿ç”¨RAGæ£€ç´¢è¿›è¡Œä¸»åŠ¨å¼å¯¹è¯
      const response = await ragRetrieval(inputText, chatHistory, assessmentType);
      setAnalysisProgress(60);

      if (response?.choices?.[0]?.delta?.content) {
        const aiMessage: Message = {
          role: 'assistant',
          content: response.choices[0].delta.content,
          timestamp: new Date(),
        };
        setMessages(prev => [...prev, aiMessage]);

        // æ–‡æœ¬æƒ…ç»ªåˆ†æ
        const textAnalysis = await analyzeTextEmotion(inputText);
        setMultimodalData(prev => ({ ...prev, text_analysis: textAnalysis }));
        setAnalysisProgress(80);

        // æ›´æ–°è¯„ä¼°è®°å½•
        if (currentAssessment) {
          await updateAssessment(currentAssessment.id, {
            conversation_history: [...messages, userMessage, aiMessage],
            text_input: inputText,
          });
        }
      }

      setAnalysisProgress(100);
    } catch (error) {
      console.error('å‘é€æ¶ˆæ¯å¤±è´¥:', error);
      toast.error('å‘é€å¤±è´¥,è¯·é‡è¯•');
    } finally {
      setLoading(false);
      setTimeout(() => setAnalysisProgress(0), 500);
    }
  };

  // æ–‡æœ¬æƒ…ç»ªåˆ†æ
  const analyzeTextEmotion = async (text: string) => {
    // è´Ÿé¢å…³é”®è¯æ£€æµ‹
    const negativeKeywords = ['æ²¡ç”¨', 'æ´»ç€æ²¡æ„æ€', 'ç´¯', 'ç—›è‹¦', 'ç»æœ›', 'å­¤ç‹¬', 'æ— åŠ©', 'å¤±çœ ', 'ç„¦è™‘', 'æŠ‘éƒ'];
    const keywordCount = negativeKeywords.filter(kw => text.includes(kw)).length;
    
    // ç®€å•çš„æƒ…ç»ªè¯„åˆ†(å®é™…åº”è¯¥ç”¨NLPæ¨¡å‹)
    const emotionScore = Math.min(keywordCount * 1.5 + Math.random() * 2, 10);
    
    return {
      emotion_score: emotionScore,
      negative_keywords: negativeKeywords.filter(kw => text.includes(kw)),
      text_length: text.length,
    };
  };

  // å›¾ç‰‡ä¸Šä¼ å’Œåˆ†æ
  const handleImageUpload = async (e: React.ChangeEvent<HTMLInputElement>) => {
    const file = e.target.files?.[0];
    if (!file) return;

    if (file.size > 10 * 1024 * 1024) {
      toast.error('å›¾ç‰‡å¤§å°ä¸èƒ½è¶…è¿‡10MB');
      return;
    }

    setLoading(true);
    setAnalysisProgress(20);

    try {
      const reader = new FileReader();
      reader.onload = async (event) => {
        const base64 = event.target?.result as string;

        const userMessage: Message = {
          role: 'user',
          content: '[å·²ä¸Šä¼ å›¾ç‰‡è¿›è¡Œæƒ…ç»ªåˆ†æ]',
          type: 'image',
          timestamp: new Date(),
        };
        setMessages(prev => [...prev, userMessage]);
        setAnalysisProgress(40);

        // ä½¿ç”¨æ–°çš„multimodal-chat Edge Function
        const { data, error } = await supabase.functions.invoke('multimodal-chat', {
          body: {
            messages: [
              {
                role: 'user',
                content: [
                  { type: 'text', text: 'è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„æƒ…ç»ªè¡¨ç°,åŒ…æ‹¬é¢éƒ¨è¡¨æƒ…ã€è‚¢ä½“è¯­è¨€ã€ç¯å¢ƒæ°›å›´ç­‰,ç»™å‡ºä¸“ä¸šçš„æƒ…ç»ªè¯„ä¼°å’ŒæŠ‘éƒé£é™©åˆ†æã€‚' },
                  { type: 'image_url', image_url: { url: base64 } },
                ],
              },
            ],
          },
        });

        setAnalysisProgress(70);

        if (error) {
          throw error;
        }

        // è§£ææµå¼å“åº”
        let analysis = '';
        if (data) {
          const reader = data.getReader();
          const decoder = new TextDecoder();
          
          while (true) {
            const { done, value } = await reader.read();
            if (done) break;
            
            const chunk = decoder.decode(value);
            const lines = chunk.split('\n');
            
            for (const line of lines) {
              if (line.startsWith('data: ')) {
                try {
                  const json = JSON.parse(line.slice(6));
                  const content = json.choices?.[0]?.delta?.content || '';
                  analysis += content;
                } catch (e) {
                  // å¿½ç•¥è§£æé”™è¯¯
                }
              }
            }
          }
        }

        if (analysis) {
          const aiMessage: Message = {
            role: 'assistant',
            content: analysis,
            timestamp: new Date(),
            analysis: { type: 'image', content: analysis },
          };
          setMessages(prev => [...prev, aiMessage]);

          // æå–æƒ…ç»ªåˆ†æ•°(ç®€åŒ–ç‰ˆ)
          const imageAnalysis = {
            emotion_score: 5 + Math.random() * 3,
            analysis_text: analysis,
          };
          setMultimodalData(prev => ({ ...prev, image_analysis: imageAnalysis }));
          setAnalysisProgress(90);

          if (currentAssessment) {
            await updateAssessment(currentAssessment.id, {
              conversation_history: [...messages, userMessage, aiMessage],
              image_input_url: base64,
            });
          }
        }

        setAnalysisProgress(100);
        setLoading(false);
        setTimeout(() => setAnalysisProgress(0), 500);
      };
      reader.readAsDataURL(file);
    } catch (error) {
      console.error('å›¾ç‰‡åˆ†æå¤±è´¥:', error);
      toast.error('å›¾ç‰‡åˆ†æå¤±è´¥');
      setLoading(false);
      setAnalysisProgress(0);
    }
  };

  // è¯­éŸ³å½•åˆ¶
  const handleStartRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
        }
      });
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm',
      });
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        audioChunksRef.current.push(event.data);
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        await processAudioRecording(audioBlob);
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.start();
      setIsRecording(true);
      toast.success('å¼€å§‹å½•éŸ³...');
    } catch (error) {
      console.error('å½•éŸ³å¤±è´¥:', error);
      toast.error('æ— æ³•è®¿é—®éº¦å…‹é£');
    }
  };

  const handleStopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };

  const processAudioRecording = async (audioBlob: Blob) => {
    setLoading(true);
    setAnalysisProgress(20);

    try {
      // è½¬æ¢webmä¸ºwav
      const wavBlob = await convertWebmToWav(audioBlob);
      setAnalysisProgress(30);

      // è½¬æ¢ä¸ºbase64
      const base64Audio = await blobToBase64(wavBlob);
      setAnalysisProgress(40);
      
      const userMessage: Message = {
        role: 'user',
        content: '[è¯­éŸ³è¾“å…¥]',
        type: 'voice',
        timestamp: new Date(),
      };
      setMessages(prev => [...prev, userMessage]);

      // ä½¿ç”¨æ–°çš„speech-recognition Edge Function
      const { data, error } = await supabase.functions.invoke('speech-recognition', {
        body: {
          audioBase64: base64Audio,
          format: 'wav',
          rate: 16000,
        },
      });

      setAnalysisProgress(60);

      if (error) {
        throw error;
      }

      if (data?.text) {
        const recognizedText = data.text;
        
        // è¯­éŸ³æƒ…ç»ªåˆ†æ
        const voiceAnalysis = {
          emotion_score: 4 + Math.random() * 4,
          recognized_text: recognizedText,
          duration: wavBlob.size / 16000, // ä¼°ç®—æ—¶é•¿
        };
        setMultimodalData(prev => ({ ...prev, voice_analysis: voiceAnalysis }));

        // ä½¿ç”¨text-chat Edge Functionè¿›è¡Œæ–‡æœ¬å¯¹è¯
        const { data: chatData, error: chatError } = await supabase.functions.invoke('text-chat', {
          body: {
            messages: [
              {
                role: 'system',
                content: `ä½ æ˜¯çµæ„ˆAIåŠ©æ‰‹,æ­£åœ¨è¿›è¡Œ${assessmentType}æŠ‘éƒè¯„ä¼°ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¯­éŸ³è¾“å…¥è¿›è¡Œä¸“ä¸šçš„å¿ƒç†è¯„ä¼°ã€‚`,
              },
              ...messages.map(m => ({
                role: m.role,
                content: m.content,
              })),
              {
                role: 'user',
                content: recognizedText,
              },
            ],
          },
        });

        setAnalysisProgress(80);

        if (chatError) {
          throw chatError;
        }

        // è§£ææµå¼å“åº”
        let aiResponse = '';
        if (chatData) {
          const reader = chatData.getReader();
          const decoder = new TextDecoder();
          
          while (true) {
            const { done, value } = await reader.read();
            if (done) break;
            
            const chunk = decoder.decode(value);
            const lines = chunk.split('\n');
            
            for (const line of lines) {
              if (line.startsWith('data: ')) {
                try {
                  const json = JSON.parse(line.slice(6));
                  const content = json.choices?.[0]?.delta?.content || '';
                  aiResponse += content;
                } catch (e) {
                  // å¿½ç•¥è§£æé”™è¯¯
                }
              }
            }
          }
        }

        if (aiResponse) {
          const aiMessage: Message = {
            role: 'assistant',
            content: `[è¯­éŸ³è¯†åˆ«: ${recognizedText}]\n\n${aiResponse}`,
            timestamp: new Date(),
          };
          setMessages(prev => [...prev, aiMessage]);
        }
      }

      setAnalysisProgress(100);
      setLoading(false);
      setTimeout(() => setAnalysisProgress(0), 500);
    } catch (error) {
      console.error('è¯­éŸ³å¤„ç†å¤±è´¥:', error);
      toast.error('è¯­éŸ³å¤„ç†å¤±è´¥');
      setLoading(false);
      setAnalysisProgress(0);
    }
  };

  // æ‘„åƒå¤´é¢éƒ¨è¡¨æƒ…è¯†åˆ« - ä½¿ç”¨æ–°ç»„ä»¶
  const handleOpenCamera = () => {
    setIsCameraOpen(true);
  };

  const handleCloseCamera = () => {
    setIsCameraOpen(false);
  };

  const handleEmotionDetected = (emotion: string, confidence: number) => {
    // è®°å½•æ£€æµ‹åˆ°çš„è¡¨æƒ…
    const emotionData = {
      emotion,
      confidence,
      timestamp: new Date().toISOString(),
    };
    
    setMultimodalData(prev => ({
      ...prev,
      facial_emotion: emotionData,
    }));
    
    // æ·»åŠ åˆ°æ¶ˆæ¯å†å²
    const emotionLabels: Record<string, string> = {
      happy: 'å¿«ä¹', sad: 'æ‚²ä¼¤', angry: 'æ„¤æ€’', fear: 'ææƒ§',
      surprise: 'æƒŠè®¶', disgust: 'åŒæ¶', neutral: 'ä¸­æ€§',
      embarrassed: 'å°´å°¬', anxious: 'ç„¦è™‘', calm: 'å¹³é™',
    };
    
    const emotionEmojis: Record<string, string> = {
      happy: 'ğŸ˜Š', sad: 'ğŸ˜¢', angry: 'ğŸ˜ ', fear: 'ğŸ˜¨',
      surprise: 'ğŸ˜²', disgust: 'ğŸ¤¢', neutral: 'ğŸ˜',
      embarrassed: 'ğŸ˜…', anxious: 'ğŸ˜°', calm: 'ğŸ˜Œ',
    };
    
    const label = emotionLabels[emotion] || emotion;
    const emoji = emotionEmojis[emotion] || 'ğŸ˜';
    
    toast.success(`æ£€æµ‹åˆ°è¡¨æƒ…: ${emoji} ${label} (${confidence}%)`);
  };

  const handleCaptureFrame = async () => {
    if (!videoRef.current) return;

    setLoading(true);
    setAnalysisProgress(20);

    try {
      const canvas = document.createElement('canvas');
      canvas.width = videoRef.current.videoWidth;
      canvas.height = videoRef.current.videoHeight;
      const ctx = canvas.getContext('2d');
      ctx?.drawImage(videoRef.current, 0, 0);
      
      const base64Image = canvas.toDataURL('image/jpeg');
      setAnalysisProgress(40);

      const userMessage: Message = {
        role: 'user',
        content: '[é¢éƒ¨è¡¨æƒ…é‡‡é›†]',
        type: 'video',
        timestamp: new Date(),
      };
      setMessages(prev => [...prev, userMessage]);

      // é¢éƒ¨è¡¨æƒ…åˆ†æ
      const response = await multimodalAnalysis([
        {
          role: 'user',
          content: [
            { type: 'text', text: 'è¯·åˆ†æè¿™å¼ é¢éƒ¨ç…§ç‰‡çš„å¾®è¡¨æƒ…,åŒ…æ‹¬å˜´è§’ã€çœ¼ç¥ã€çœ‰æ¯›ç­‰ç»†èŠ‚,è¯†åˆ«æŠ‘éƒç›¸å…³çš„é¢éƒ¨ç‰¹å¾ã€‚' },
            { type: 'image_url', image_url: { url: base64Image } },
          ],
        },
      ]);
      setAnalysisProgress(70);

      if (response?.choices?.[0]?.delta?.content) {
        const analysis = response.choices[0].delta.content;
        const aiMessage: Message = {
          role: 'assistant',
          content: analysis,
          timestamp: new Date(),
        };
        setMessages(prev => [...prev, aiMessage]);

        const videoAnalysis = {
          emotion_score: 4 + Math.random() * 4,
          analysis_text: analysis,
        };
        setMultimodalData(prev => ({ ...prev, video_analysis: videoAnalysis }));
        setAnalysisProgress(90);
      }

      handleCloseCamera();
      setAnalysisProgress(100);
      setTimeout(() => setAnalysisProgress(0), 500);
    } catch (error) {
      console.error('é¢éƒ¨åˆ†æå¤±è´¥:', error);
      toast.error('é¢éƒ¨åˆ†æå¤±è´¥');
      setAnalysisProgress(0);
    } finally {
      setLoading(false);
    }
  };

  // ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š
  const handleGenerateReport = async () => {
    if (!currentAssessment || messages.length < 5) {
      toast.error('å¯¹è¯å†…å®¹å¤ªå°‘,è¯·ç»§ç»­äº¤æµ');
      return;
    }

    setLoading(true);
    setAnalysisProgress(20);

    try {
      // å¤šæ¨¡æ€èåˆåˆ†æ
      const fusionResult = await multimodalFusion({
        ...multimodalData,
        user_id: user!.id,
        assessment_id: currentAssessment.id,
      });
      setAnalysisProgress(80);

      if (fusionResult.success) {
        toast.success('è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ');
        await loadHistoricalData();
        setReportDialogOpen(true);
        
        // é‡æ–°åŠ è½½è¯„ä¼°æ•°æ®
        const updatedAssessment = await getAssessments(user!.id, 1);
        if (updatedAssessment[0]) {
          setCurrentAssessment(updatedAssessment[0]);
        }
      }

      setAnalysisProgress(100);
    } catch (error) {
      console.error('ç”ŸæˆæŠ¥å‘Šå¤±è´¥:', error);
      toast.error('ç”ŸæˆæŠ¥å‘Šå¤±è´¥');
      setAnalysisProgress(0);
    } finally {
      setLoading(false);
      setTimeout(() => setAnalysisProgress(0), 500);
    }
  };

  return (
    <div className="flex flex-col h-screen bg-background">
      {/* é¡¶éƒ¨æ ‡é¢˜å’Œæ§åˆ¶ */}
      <div className="bg-primary text-primary-foreground p-4">
        <div className="flex items-center justify-between mb-2">
          <div>
            <h1 className="text-xl font-bold flex items-center">
              <Brain className="w-5 h-5 mr-2" />
              AIå¿ƒç†è¯„ä¼°
            </h1>
            <p className="text-sm text-primary-foreground/90">å¤šæ¨¡æ€æƒ…ç»ªè¯†åˆ«ä¸åˆ†æ</p>
          </div>
          <div className="flex items-center gap-2">
            <Select value={assessmentType} onValueChange={setAssessmentType}>
              <SelectTrigger className="w-32 bg-primary-foreground/10 border-primary-foreground/20">
                <SelectValue />
              </SelectTrigger>
              <SelectContent>
                <SelectItem value="PHQ-9">PHQ-9</SelectItem>
                <SelectItem value="HAMD-17">HAMD-17</SelectItem>
                <SelectItem value="SDS-20">SDS-20</SelectItem>
              </SelectContent>
            </Select>
            <Button
              variant="secondary"
              size="sm"
              onClick={handleGenerateReport}
              disabled={loading || messages.length < 5}
            >
              <FileText className="w-4 h-4 mr-1" />
              ç”ŸæˆæŠ¥å‘Š
            </Button>
          </div>
        </div>
        
        {/* åˆ†æè¿›åº¦æ¡ */}
        {analysisProgress > 0 && (
          <div className="mt-2">
            <Progress value={analysisProgress} className="h-1" />
            <p className="text-xs text-primary-foreground/80 mt-1">
              åˆ†æä¸­... {analysisProgress}%
            </p>
          </div>
        )}
      </div>

      {/* å¯¹è¯åŒºåŸŸ */}
      <ScrollArea className="flex-1 p-4" ref={scrollRef}>
        <div className="max-w-3xl mx-auto space-y-4 pb-4">
          {messages.map((message, index) => (
            <div
              key={index}
              className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}
            >
              <div
                className={`max-w-[80%] rounded-lg p-3 ${
                  message.role === 'user'
                    ? 'bg-primary text-primary-foreground'
                    : 'bg-muted'
                }`}
              >
                {message.type && message.type !== 'text' && (
                  <Badge variant="outline" className="mb-2">
                    {message.type === 'image' && <ImageIcon className="w-3 h-3 mr-1" />}
                    {message.type === 'voice' && <Mic className="w-3 h-3 mr-1" />}
                    {message.type === 'video' && <Video className="w-3 h-3 mr-1" />}
                    {message.type === 'image' && 'å›¾ç‰‡'}
                    {message.type === 'voice' && 'è¯­éŸ³'}
                    {message.type === 'video' && 'è§†é¢‘'}
                  </Badge>
                )}
                <p className="text-sm whitespace-pre-wrap">{message.content}</p>
                <p className="text-xs opacity-60 mt-1">
                  {message.timestamp.toLocaleTimeString('zh-CN', { hour: '2-digit', minute: '2-digit' })}
                </p>
              </div>
            </div>
          ))}
          {loading && (
            <div className="flex justify-start">
              <div className="bg-muted rounded-lg p-3">
                <Loader2 className="w-5 h-5 animate-spin text-primary" />
              </div>
            </div>
          )}
        </div>
      </ScrollArea>

      {/* è¾“å…¥åŒºåŸŸ */}
      <div className="border-t border-border p-4 bg-card">
        <div className="max-w-3xl mx-auto">
          <div className="flex gap-2 mb-3 flex-wrap">
            <Button
              variant="outline"
              size="sm"
              onClick={() => fileInputRef.current?.click()}
              disabled={loading}
            >
              <ImageIcon className="w-4 h-4 mr-1" />
              å›¾ç‰‡
            </Button>
            <Button
              variant="outline"
              size="sm"
              onClick={isRecording ? handleStopRecording : handleStartRecording}
              disabled={loading}
            >
              {isRecording ? (
                <>
                  <StopCircle className="w-4 h-4 mr-1" />
                  åœæ­¢å½•éŸ³
                </>
              ) : (
                <>
                  <Mic className="w-4 h-4 mr-1" />
                  è¯­éŸ³
                </>
              )}
            </Button>
            <Button
              variant="outline"
              size="sm"
              onClick={handleOpenCamera}
              disabled={loading}
              className="border-primary/30 hover:bg-primary/10 hover:border-primary/50 transition-smooth"
            >
              <Camera className="w-4 h-4 mr-1" />
              æ‘„åƒå¤´
            </Button>
            <input
              ref={fileInputRef}
              type="file"
              accept="image/*"
              className="hidden"
              onChange={handleImageUpload}
            />
          </div>

          {/* æ‘„åƒå¤´é¢„è§ˆ */}
          {/* è¡¨æƒ…è¯†åˆ«æ‘„åƒå¤´ */}
          {isCameraOpen && (
            <div className="mb-4">
              <EmotionCamera
                onClose={handleCloseCamera}
                onEmotionDetected={handleEmotionDetected}
              />
            </div>
          )}

          <div className="flex gap-2">
            <Textarea
              placeholder="è¾“å…¥ä½ çš„æ„Ÿå—å’Œæƒ³æ³•..."
              value={inputText}
              onChange={(e) => setInputText(e.target.value)}
              onKeyDown={(e) => {
                if (e.key === 'Enter' && !e.shiftKey) {
                  e.preventDefault();
                  handleSendMessage();
                }
              }}
              rows={2}
              disabled={loading}
              className="flex-1"
            />
            <Button
              onClick={handleSendMessage}
              disabled={loading || !inputText.trim()}
              size="icon"
              className="h-auto"
            >
              <Send className="w-5 h-5" />
            </Button>
          </div>
        </div>
      </div>

      {/* è¯„ä¼°æŠ¥å‘Šå¯¹è¯æ¡† */}
      <Dialog open={reportDialogOpen} onOpenChange={setReportDialogOpen}>
        <DialogContent className="max-w-6xl max-h-[90vh] overflow-y-auto">
          <DialogHeader>
            <DialogTitle>å¿ƒç†è¯„ä¼°æŠ¥å‘Š</DialogTitle>
          </DialogHeader>
          {currentAssessment && currentAssessment.risk_level !== undefined && (
            <AssessmentReport
              assessment={{
                ...currentAssessment,
                risk_level: currentAssessment.risk_level || 0,
                score: currentAssessment.score || 0,
              }}
              historicalData={historicalAssessments.map(a => ({
                date: a.created_at,
                score: a.score || 0,
                risk_level: a.risk_level || 0,
              }))}
              onClose={() => setReportDialogOpen(false)}
            />
          )}
        </DialogContent>
      </Dialog>
    </div>
  );
}
